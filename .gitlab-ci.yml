image: java:latest

stages:
  - build
  - database
  - docker
  - deploy

maven-build:
  image: maven:3-jdk-8
  stage: build
  script:
    - echo "Building Jar File"
    - mvn clean install
  artifacts:
    paths:
      - pdx-gun.jar
    expire_in: 1 day


init-database:
  stage: database
  script:
    - echo "Initializing Postgres Database"
    - /usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar pdx-gun.jar
  artifacts:
    paths:
      - init.sql
    expire_in: 1 day


docker-build:
  # Official docker image.
  image: docker:latest
  variables:
    # For EBI you need to override the definition of CI_REGISTRY to remove the port number
    CI_REGISTRY: dockerhub.ebi.ac.uk
    CI_REGISTRY_IMAGE: $CI_REGISTRY/$CI_PROJECT_PATH
    # set docker driver for speed
    DOCKER_DRIVER: overlay2
    # Solve issue: docker: Cannot connect to the Docker daemon at tcp://docker:2375. Is the docker daemon running?
    DOCKER_TLS_CERTDIR: ""

  stage: docker
  services:
    - docker:dind
  before_script:
    - echo $CI_BUILD_TOKEN | docker login --username "$CI_REGISTRY_USER" --password-stdin $CI_REGISTRY
    - NOW=$(date '+%Y-%m-%d-%H-%M-%S')-"$CI_COMMIT_REF_NAME"
  script:
    - echo "${CI_REGISTRY_IMAGE}":"${NOW}"

    - docker build --pull -t "$CI_REGISTRY_IMAGE":"${NOW}" .
    #- docker commit refdbcontainer "${CI_REGISTRY_IMAGE}":"${NOW}"
    - docker tag "${CI_REGISTRY_IMAGE}":"${NOW}" "${CI_REGISTRY_IMAGE}":latest
    - docker push "$CI_REGISTRY_IMAGE"

    - docker logout ${CI_REGISTRY}


    # PUSH THE IMAGE TO DOCKER HUB
    - echo "${DOCKER_HUB_PWD}" | docker login --username "${DOCKER_HUB_USER}" --password-stdin docker.io

    - docker tag "${CI_REGISTRY_IMAGE}":"${NOW}" "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":"${NOW}"
    - docker tag "${CI_REGISTRY_IMAGE}":"${NOW}" "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":latest
    - docker push "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}"

    - docker logout


deploy-production:
  stage: deploy
  image: dtzar/helm-kubectl:2.13.0
  script:
    - kubectl config set-cluster local --server="${KUBERNETES_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
    - kubectl config set-credentials pdx-gun-admin --token="${KUBERNETES_USER_TOKEN}"
    - kubectl config set-context "${KUBERNETES_NAMESPACE}" --cluster=local --user=${KUBERNETES_USER} --namespace="${KUBERNETES_NAMESPACE}"
    - kubectl config use-context "${KUBERNETES_NAMESPACE}"
    - kubectl version

    - sed -i "s/latest/${NOW}/g" k8-deploy/database/deployment.yml

    - kubectl apply -f k8-deploy/database/deployment.yml --record
    - kubectl rollout status -f k8-deploy/database/deployment.yml

    - kubectl apply -f k8-deploy/database/service.yml --record

    - kubectl apply -f k8-deploy/hasura/prod/hasura-deployment.yml --record
    - kubectl rollout status -f k8-deploy/hasura/prod/hasura-deployment.yml

    - kubectl apply -f k8-deploy/hasura/prod/hasura-service.yml --record
    - kubectl apply -f k8-deploy/hasura/prod/hasura-ingress.yml --record

    - kubectl get pods,service,deploy,replicaset,ing
  only:
    refs:
      - master

deploy-HH-WP-WEBADMIN:
  stage: deploy
  image: dtzar/helm-kubectl:2.13.0
  script:
    - kubectl config set-cluster local --server="${HH_WP_WEBADMIN_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${HH_WP_WEBADMIN_CERTIFICATE_AUTHORITY}"
    - kubectl config set-credentials pdx-gun-admin --token="${HH_WP_WEBADMIN_USER_TOKEN}"
    - kubectl config set-context "${HH_WP_WEBADMIN_NAMESPACE}" --cluster=local --user=${HH_WP_WEBADMIN_USER} --namespace="${HH_WP_WEBADMIN_NAMESPACE}"
    - kubectl config use-context "${HH_WP_WEBADMIN_NAMESPACE}"
    - kubectl version

    - sed -i "s/latest/${NOW}/g" k8-deploy/database/deployment.yml

    - kubectl apply -f k8-deploy/database/deployment.yml --record
    - kubectl rollout status -f k8-deploy/database/deployment.yml

    - kubectl apply -f k8-deploy/database/service.yml --record

    - kubectl apply -f k8-deploy/hasura/dev/hasura-deployment.yml --record
    - kubectl rollout status -f k8-deploy/hasura/dev/hasura-deployment.yml

    - kubectl apply -f k8-deploy/hasura/dev/hasura-service.yml --record
    - kubectl apply -f k8-deploy/hasura/dev/hasura-ingress.yml --record

    - kubectl get pods,service,deploy,replicaset,ing
  only:
    refs:
      - dev
